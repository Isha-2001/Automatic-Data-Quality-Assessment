{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd0b89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from hyperloglog import HyperLogLog\n",
    "from nltk import ngrams\n",
    "from scipy.stats import skew, kurtosis\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "from dash.dependencies import Input, Output, State\n",
    "import os\n",
    "from dabl import detect_types\n",
    "from hyperloglog import HyperLogLog\n",
    "from collections import namedtuple, Counter as count\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from hyperloglog import HyperLogLog\n",
    "from pyod.models.knn import KNN\n",
    "from abc import abstractmethod\n",
    "from dabl import detect_types\n",
    "from nltk.util import ngrams\n",
    "from pathlib import Path\n",
    "from enum import IntEnum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "import math\n",
    "import plotly.express as px\n",
    "from urllib.parse import unquote\n",
    "from dash import Dash, dcc, html, Input, Output, State, callback_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b038f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data loader.py \n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pdaoao\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import namedtuple, Counter as count\n",
    "\n",
    "root_directory_path = 'C:/Users/Isha/ERP/DataRepo2/'\n",
    "\n",
    "def populate_directory_options(directory_path):\n",
    "    options = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for dir_name in dirs:\n",
    "            options.append({'label': os.path.join(root, dir_name), 'value': os.path.join(root, dir_name)})\n",
    "    return options\n",
    "\n",
    "def detect_header(file_path, delimiter=','):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter, nrows=2)\n",
    "        if df.empty:\n",
    "            return False\n",
    "        first_row = df.iloc[0]\n",
    "        second_row = df.iloc[1]\n",
    "        if first_row.dtype == object and second_row.dtype != object:\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "#         print(f\"Failed to read {file_path} to detect header: {e}\")\n",
    "        return False\n",
    "\n",
    "def DetectHeader(df):\n",
    "    if df is None:\n",
    "            return False\n",
    "    first_row = df.iloc[0]\n",
    "    second_row = df.iloc[1]\n",
    "    if first_row.dtype == object and second_row.dtype != object:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def try_read_csv(file_path, delimiter=','):\n",
    "    try:\n",
    "        if detect_header(file_path, delimiter):\n",
    "            return pd.read_csv(file_path, delimiter=delimiter)\n",
    "        else:\n",
    "            return pd.read_csv(file_path, delimiter=delimiter, header=None)\n",
    "    except Exception as e:\n",
    "#         print(f\"Failed to read {file_path} as CSV with delimiter '{delimiter}': {e}\")\n",
    "        return None\n",
    "\n",
    "def try_read_json_lines(file_path):\n",
    "    try:\n",
    "        return pd.read_json(file_path, lines=True)\n",
    "    except Exception as e:\n",
    "#         print(f\"Failed to read {file_path} as JSON Lines: {e}\")\n",
    "        return None\n",
    "\n",
    "def try_read_text_as_json(file_path):\n",
    "    try:\n",
    "        records = []\n",
    "        with open(file_path, 'r') as f:\n",
    "            for line in f:\n",
    "                records.append(json.loads(line.strip()))\n",
    "        return pd.DataFrame(records)\n",
    "    except Exception as e:\n",
    "#         print(f\"Failed to read {file_path} as JSON per line: {e}\")\n",
    "        return None\n",
    "\n",
    "def try_read_text_generic(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        delimiters = [',', '\\t', ' ']\n",
    "        for delimiter in delimiters:\n",
    "            try:\n",
    "                has_header = detect_header(file_path, delimiter)\n",
    "                if has_header:\n",
    "                    df = pd.read_csv(file_path, delimiter=delimiter)\n",
    "                else:\n",
    "                    df = pd.read_csv(file_path, delimiter=delimiter, header=None)\n",
    "                return df\n",
    "            except Exception as e:\n",
    "#                 print(f\"Failed to parse {file_path} with delimiter '{delimiter}': {e}\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "#         print(f\"Failed to read {file_path} as generic text: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_files(root_directory_path):\n",
    "    dataframes_dict = {}\n",
    "    for dirpath, dirnames, filenames in os.walk(root_directory_path):\n",
    "        for file in filenames:\n",
    "            file_path = os.path.join(dirpath, file)\n",
    "            df = None\n",
    "            if file.endswith('.csv'):\n",
    "                df = pd.read_csv(file_path)\n",
    "            elif file.endswith('.xlsx') or file.endswith('.xls'):\n",
    "                df = pd.read_excel(file_path)\n",
    "            elif file.endswith('.json'):\n",
    "                df = pd.read_json(file_path)\n",
    "            elif file.endswith('.jsonl'):\n",
    "                df = pd.read_json(file_path, lines=True)\n",
    "            elif file.endswith('.data'):\n",
    "                df = pd.read_csv(file_path, delimiter=\",\")\n",
    "            else:\n",
    "                df = try_read_text_as_json(file_path)\n",
    "                if df is None:\n",
    "                    df = try_read_csv(file_path, delimiter=',')\n",
    "                if df is None:\n",
    "                    df = try_read_csv(file_path, delimiter='\\t')\n",
    "                if df is None:\n",
    "                    df = try_read_text_generic(file_path)\n",
    "            if df is not None:\n",
    "                dataframes_dict[file_path] = df\n",
    "    return dataframes_dict\n",
    "\n",
    "\n",
    "def load_single_file(file_path):\n",
    "    df = None\n",
    "    try:\n",
    "        if file_path.endswith('.csv'):\n",
    "            df = pd.read_csv(file_path)\n",
    "        elif file_path.endswith('.xlsx') or file_path.endswith('.xls'):\n",
    "            df = pd.read_excel(file_path)\n",
    "        elif file_path.endswith('.json'):\n",
    "            df = pd.read_json(file_path)\n",
    "        elif file_path.endswith('.jsonl'):\n",
    "            df = pd.read_json(file_path, lines=True)\n",
    "        elif file_path.endswith('.data'):\n",
    "            df = pd.read_csv(file_path, delimiter=\",\")\n",
    "        elif file_path.endswith('.txt'):\n",
    "            df = try_read_text_as_json(file_path)\n",
    "            if df is None:\n",
    "                df = try_read_csv(file_path, delimiter=',')\n",
    "            if df is None:\n",
    "                df = try_read_csv(file_path, delimiter='\\t')\n",
    "            if df is None:\n",
    "                df = try_read_text_generic(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "def dataType_distribution(df):\n",
    "    data_types = df.dtypes.value_counts()\n",
    "    return data_types.to_json()\n",
    "\n",
    "\n",
    "def overall_outlier_percentage(df):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number])\n",
    "    if numeric_cols.empty:\n",
    "        return 0.0\n",
    "    outliers = ((numeric_cols < (numeric_cols.quantile(0.25) - 1.5 * (numeric_cols.quantile(0.75) - numeric_cols.quantile(0.25)))) |\n",
    "                (numeric_cols > (numeric_cols.quantile(0.75) + 1.5 * (numeric_cols.quantile(0.75) - numeric_cols.quantile(0.25))))).sum()\n",
    "    total_values = numeric_cols.count()\n",
    "    outlier_percent = (outliers.sum() / total_values.sum()) * 100\n",
    "    return outlier_percent\n",
    "\n",
    "def calculate_data_quality_dynamic(completeness_score, outlier_percentage, uniqueness_score, rows, columns):\n",
    "    # Dynamic weight adjustment based on shape\n",
    "    if rows > 10000 or columns > 100:  # Large datasets\n",
    "        completeness_weight = 0.3\n",
    "        outlier_weight = 0.4\n",
    "        uniqueness_weight = 0.2\n",
    "        shape_weight = 0.1\n",
    "    elif rows < 100 and columns < 10:  # Small datasets\n",
    "        completeness_weight = 0.4\n",
    "        outlier_weight = 0.2\n",
    "        uniqueness_weight = 0.3\n",
    "        shape_weight = 0.1\n",
    "    else:  # Medium datasets\n",
    "        completeness_weight = 0.3\n",
    "        outlier_weight = 0.3\n",
    "        uniqueness_weight = 0.3\n",
    "        shape_weight = 0.1\n",
    "    # Define thresholds for optimal, too sparse, and too complex\n",
    "    optimal_row_lower = 100\n",
    "    optimal_row_upper = 10000\n",
    "    optimal_column_lower = 5\n",
    "    optimal_column_upper = 50\n",
    "    \n",
    "    # Calculate shape bonus/penalty\n",
    "    shape_bonus = 0\n",
    "    \n",
    "    # Bonus for optimal number of rows and columns\n",
    "    if optimal_row_lower <= rows <= optimal_row_upper and optimal_column_lower <= columns <= optimal_column_upper:\n",
    "        shape_bonus += 10  # Give bonus points for optimal shape\n",
    "    else:\n",
    "        # Penalty for too sparse or too complex datasets\n",
    "        if rows < optimal_row_lower or columns < optimal_column_lower:\n",
    "            shape_bonus -= 10  # Penalty for too sparse datasets\n",
    "        if rows > optimal_row_upper or columns > optimal_column_upper:\n",
    "            shape_bonus -= 10  # Penalty for too complex datasets\n",
    "\n",
    "    \n",
    "    # Calculate data quality percent with dynamic weighting\n",
    "    data_quality = (completeness_score * completeness_weight +\n",
    "                    (100 - outlier_percentage) * outlier_weight +\n",
    "                    uniqueness_score * uniqueness_weight +\n",
    "                    shape_bonus*shape_weight)\n",
    "    data_quality_percentage = max(0, min(100, data_quality))\n",
    "\n",
    "    return data_quality_percentage #score\n",
    "\n",
    "# Check for columns containing lists and convert them to tuples\n",
    "def convert_lists_to_tuples(row):\n",
    "    return [tuple(item) if isinstance(item, list) else item for item in row]\n",
    "\n",
    "\n",
    "\n",
    "def analyze_files(root_directory_path):\n",
    "    dataframes_dict = load_files(root_directory_path)\n",
    "#     print(dataframes_dict)\n",
    "    data = []\n",
    "\n",
    "    for file_path, df in dataframes_dict.items():\n",
    "        file_name = os.path.basename(file_path)\n",
    "        size_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "        rows, columns = df.shape\n",
    "#         print(df)\n",
    "        completeness_score = (1 - df.isnull().mean().mean()) * 100\n",
    "        outlier_percentage = overall_outlier_percentage(df)\n",
    "        # Apply the conversion to the entire DataFrame\n",
    "        df_converted = df.apply(convert_lists_to_tuples, axis=1)\n",
    "        uniqueness_score = (df_converted.drop_duplicates().shape[0] / rows) * 100\n",
    "        data_type_distribution = dataType_distribution(df)\n",
    "        data_quality_percent = calculate_data_quality_dynamic(completeness_score, outlier_percentage, uniqueness_score, rows, columns)\n",
    "\n",
    "        data.append({\n",
    "            'name': file_name,\n",
    "            'size (MB)': size_mb,\n",
    "            'rows': rows,\n",
    "            'columns': columns,\n",
    "            'completeness_score': completeness_score,\n",
    "            'outlier_percentage': outlier_percentage,\n",
    "            'uniqueness_score': uniqueness_score,\n",
    "            'data_type_distribution': data_type_distribution,\n",
    "            'data_quality_percent': data_quality_percent,\n",
    "            'file_path': file_path  # Store file path for later use\n",
    "        })\n",
    "\n",
    "    df_summary = pd.DataFrame(data)\n",
    "    print(\"Columns in DataFrame:\", df_summary.columns)\n",
    "#     print(\"Data in DataFrame:\", df_summary.head())\n",
    "    df_summary['volume'] = df_summary['rows'].astype(str) + ' x ' + df_summary['columns'].astype(str)\n",
    "    df_summary = df_summary.sort_values('data_quality_percent', ascending=False).reset_index(drop=True)\n",
    "    df_summary['ranking'] = df_summary.index + 1\n",
    "    # Extract file extensions from the file names and add a new column 'extension'\n",
    "    df_summary['extension'] = df_summary['name'].apply(lambda x: os.path.splitext(x)[1].lstrip('.').lower())\n",
    "\n",
    "#     print(profile)\n",
    "    return df_summary\n",
    "#     return dataframes_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a134f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data profiler \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class DataProfiler:\n",
    "    def __init__(self, df, column):\n",
    "        self.df = df  # The DataFrame containing the data\n",
    "        self.column = column  # The specific column to profile\n",
    "\n",
    "    def completeness_score(self):\n",
    "        completeness = self.df[self.column].notnull().sum() / self.df[self.column].shape[0] * 100\n",
    "        return completeness\n",
    "\n",
    "    def uniqueness_score(self):\n",
    "        uniqueness = self.df[self.column].nunique() / self.df[self.column].count() * 100\n",
    "        return uniqueness\n",
    "\n",
    "    def data_quality_percent(self):\n",
    "        data_quality = 100 - self.total_error_percent()\n",
    "        return data_quality\n",
    "    \n",
    "    def get_outliers(self,df, column):\n",
    "#         print(\"entered nested func\")\n",
    "        if pd.api.types.is_numeric_dtype(df[column]): \n",
    "        # IQR method for outlier detection\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            is_outlier = (df[column] < lower_bound) | (df[column] > upper_bound)\n",
    "            outliers = df[is_outlier].copy()\n",
    "            outliers['Row No. (index)'] = outliers.index\n",
    "            outliers['Actual Value'] = outliers[column]\n",
    "            outliers['Expected Value (Mean)'] = df[column].mean()\n",
    "            return outliers.reset_index(drop=True)[['Row No. (index)', 'Actual Value', 'Expected Value (Mean)']]\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    def get_rare_values(self,df, column, threshold):\n",
    "        # Convert lists to tuples for hashable operation, if applicable\n",
    "        df[column] = df[column].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n",
    "        # Calculate the frequency of each value in the column\n",
    "        value_counts = df[column].value_counts(normalize=True)\n",
    "#         print(\"HELLLOOOOOOOO freq 1\")\n",
    "\n",
    "        # Identify values that occur less frequently than the threshold\n",
    "        rare_values = value_counts[value_counts < threshold].index\n",
    "#         print(\"HELLLOOOOOOOO freq 2\")\n",
    "#         print(rare_values)\n",
    "\n",
    "        # Return the DataFrame of outliers\n",
    "        outliers = df[df[column].isin(rare_values)].copy()\n",
    "        outliers['Row No. (index)'] = outliers.index\n",
    "        outliers['Actual Value'] = outliers[column]\n",
    "        mode_series = df[column].mode()\n",
    "        if not mode_series.empty:\n",
    "            outliers['Expected Value (Mode)'] = mode_series.iloc[0]\n",
    "        else:\n",
    "            outliers['Expected Value (Mode)'] = None  # or some default value            \n",
    "        return outliers.reset_index(drop=True)[['Row No. (index)', 'Actual Value', 'Expected Value (Mode)']]\n",
    "\n",
    "    def detect_outliers_iqr(self, df, column):\n",
    "#         print(\"entered IQR\")\n",
    "        df_extended, extended_columns = extend_dataframe_with_rules(df, column)\n",
    "        outlier_list = []\n",
    "        \n",
    "        nan_columns = df_extended.columns[df_extended.isna().all()].tolist()\n",
    "        df_extended = df_extended.dropna(how='all', axis=1)  # Drop extended columns where all values are NaN\n",
    "        extended_columns = [col for col in extended_columns if col not in nan_columns]  # Update extended_columns\n",
    "        \n",
    "        # Detect outliers in original column\n",
    "#         print(\"right before enterng nested func\")\n",
    "        outliers_original = self.get_outliers(df, column)\n",
    "#         print(\"HELLLOOOOOOOO IQR\")\n",
    "#         print(outliers_orignial)\n",
    "        outlier_list.append(outliers_original)\n",
    "\n",
    "        # Detect outliers in extended columns\n",
    "        for col in extended_columns:\n",
    "            if col in df_extended.columns and pd.api.types.is_numeric_dtype(df_extended[col]):\n",
    "                outliers_extended = self.get_outliers(df_extended, col)\n",
    "                outlier_list.append(outliers_extended)\n",
    "\n",
    "        # Concatenate all outliers\n",
    "        if outlier_list:\n",
    "            outlier_df = pd.concat(outlier_list, axis=0).drop_duplicates()\n",
    "            return outlier_df\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def detect_outliers_frequency(self, df, column, threshold=0.05):\n",
    "#         print(\"entered FREQ\")\n",
    "        df_extended, extended_columns = extend_dataframe_with_rules(df, column)\n",
    "        outlier_list = []\n",
    "\n",
    "        nan_columns = df_extended.columns[df_extended.isna().all()].tolist()\n",
    "        df_extended = df_extended.dropna(how='all', axis=1)  # Drop extended columns where all values are NaN\n",
    "        extended_columns = [col for col in extended_columns if col not in nan_columns]  # Update extended_columns\n",
    "        \n",
    "        \n",
    "        # Detect outliers in the original column\n",
    "        outliers_original = self.get_rare_values(df, column, threshold)\n",
    "        outlier_list.append(outliers_original)\n",
    "#         print(\"outlier list right now - \",outlier_list)\n",
    "        # Detect outliers in extended columns\n",
    "        for col in extended_columns:\n",
    "            if col in df_extended.columns:\n",
    "#                 print(col)\n",
    "                outliers_extended = self.get_rare_values(df_extended, col, threshold)\n",
    "                outlier_list.append(outliers_extended)\n",
    "\n",
    "        # Concatenate all outliers found\n",
    "        if outlier_list:\n",
    "            outlier_df = pd.concat(outlier_list, axis=0).drop_duplicates()\n",
    "            return outlier_df\n",
    "\n",
    "        # Return an empty DataFrame if no outliers are found\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    def isIndex(self):\n",
    "        # Check if column exists in DataFrame\n",
    "        if self.column not in self.df.columns:\n",
    "            raise ValueError(f\"Column '{self.column}' does not exist in the DataFrame.\")\n",
    "\n",
    "        # Check for uniqueness\n",
    "        is_unique = self.df[self.column].is_unique\n",
    "\n",
    "        # Check for missing values\n",
    "        has_no_missing = self.df[self.column].notna().all()\n",
    "\n",
    "        # Combine both conditions\n",
    "        return is_unique and has_no_missing\n",
    "    \n",
    "    def mean_median_difference(self):\n",
    "        if pd.api.types.is_numeric_dtype(self.df[self.column]):\n",
    "            mean_val = self.df[self.column].mean()\n",
    "            median_val = self.df[self.column].median()\n",
    "            difference = abs(mean_val - median_val)\n",
    "            return difference\n",
    "        return \"N/A\"\n",
    "\n",
    "    def generate_profile(self):\n",
    "        return {\n",
    "            'completeness_score': self.completeness_score(),\n",
    "            'outlier_percentage': 100,\n",
    "            'uniqueness_score': self.uniqueness_score(),\n",
    "            'data_quality_percent': self.data_quality_percent(),\n",
    "            'isIndex' : self.isIndex(),\n",
    "            'Mean/MedianImbalance' : mean_median_difference()\n",
    "            \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a656f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#error detection.py\n",
    "import re\n",
    "import unicodedata\n",
    "import email.utils\n",
    "from collections import defaultdict\n",
    "import inspect\n",
    "import time\n",
    "\n",
    "rules = defaultdict(list)\n",
    "\n",
    "def rule(rule):\n",
    "    spec = inspect.getfullargspec(rule)\n",
    "\n",
    "    if len(spec.args) != 1:\n",
    "        raise ValueError(\"Invalid rule {}\".format(rule.__name__))\n",
    "\n",
    "    input_type = spec.annotations[spec.args[0]]\n",
    "    rules[input_type].append(rule)\n",
    "    return rule\n",
    "\n",
    "def apply_rules(df, column):\n",
    "    data_type = df[column].dtype\n",
    "    results = []\n",
    "    extended_columns = []\n",
    "    \n",
    "    if pd.api.types.is_string_dtype(data_type):\n",
    "        values = df[column].dropna()\n",
    "        for value in values:\n",
    "            result = {}\n",
    "            for rule in rules.get(str, []):\n",
    "                rule_result = rule(value)\n",
    "                result[rule.__name__] = rule_result\n",
    "                for idx, res in enumerate(rule_result):\n",
    "                    extended_columns.append(f\"{column}_{rule.__name__}_{idx}\")\n",
    "            results.append(result)\n",
    "    \n",
    "    elif pd.api.types.is_numeric_dtype(data_type):\n",
    "        values = df[column].dropna()\n",
    "        for value in values:\n",
    "            result = {}\n",
    "            for rule in rules.get(float, []):\n",
    "                rule_result = rule(value)\n",
    "                result[rule.__name__] = rule_result\n",
    "                for idx, res in enumerate(rule_result):\n",
    "                    extended_columns.append(f\"{column}_{rule.__name__}_{idx}\")\n",
    "            results.append(result)\n",
    "    \n",
    "    return results, extended_columns\n",
    "\n",
    "def extend_dataframe_with_rules(df, column):\n",
    "    rule_results, extended_columns = apply_rules(df, column)\n",
    "    \n",
    "    # Convert list of dictionaries to DataFrame\n",
    "    rule_df = pd.DataFrame(rule_results)\n",
    "    \n",
    "    # Convert boolean columns to numeric\n",
    "    for col in rule_df.columns:\n",
    "        if rule_df[col].dtype == 'bool':\n",
    "            rule_df[col] = rule_df[col].astype(int)\n",
    "    \n",
    "    # Create extended columns with default NaN values\n",
    "    for ext_col in extended_columns:\n",
    "        if ext_col not in rule_df.columns:\n",
    "            rule_df[ext_col] = pd.NA\n",
    "    \n",
    "    # Merge with original DataFrame\n",
    "    df_extended = pd.concat([df, rule_df], axis=1)\n",
    "    return df_extended, extended_columns\n",
    "\n",
    "\n",
    "@rule\n",
    "def string_case(s: str) -> (\"upper case\", \"lower case\", \"title case\"):\n",
    "    return (s.isupper(), s.islower(), s.istitle())\n",
    "\n",
    "@rule\n",
    "def string_is_digit(s: str) -> (\"is digit\",):\n",
    "    return (s.isdigit(),)\n",
    "\n",
    "@rule\n",
    "def length(s: str) -> (\"length\",):\n",
    "    return (len(s),)\n",
    "\n",
    "@rule\n",
    "def signature(s: str) -> (\"signature\",):\n",
    "    return (\",\".join(map(unicodedata.category, s)),)\n",
    "\n",
    "NUMBERS = re.compile(r\"(^s)?\\d+\")\n",
    "\n",
    "@rule\n",
    "def strp(s: str) -> (\"strp\",):\n",
    "    return (NUMBERS.sub(\"<num>\", s),)\n",
    "\n",
    "HTML5_EMAIL_VALIDATOR = re.compile(r\"^[a-zA-Z0-9.!#$%&'*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\.(?P<ext>[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?))*$\")\n",
    "\n",
    "@rule\n",
    "def email_checks(s: str) -> (\"simple email check\",):\n",
    "    return (HTML5_EMAIL_VALIDATOR.match(s) != None,)\n",
    "\n",
    "@rule\n",
    "def email_domain(s: str) -> (\"email domain\",):\n",
    "    match = HTML5_EMAIL_VALIDATOR.match(s)\n",
    "    return (match.group(\"ext\").lower() if match else \"NONE\",)\n",
    "\n",
    "@rule\n",
    "def id(s: str) -> (\"id\",):\n",
    "    return (s,)\n",
    "\n",
    "@rule\n",
    "def empty(s: str) -> (\"empty\",):\n",
    "    return (s == \"\" or s.isspace(),)\n",
    "\n",
    "@rule\n",
    "def int_id(x: int) -> (\"id\",):\n",
    "    return (x,)\n",
    "\n",
    "@rule\n",
    "def int_kill(x: int) -> (\"nil\",):\n",
    "    return (None,)\n",
    "\n",
    "@rule\n",
    "def float_id(f: float) -> (\"id\",):\n",
    "    return (f,)\n",
    "\n",
    "def _bits(*positions):\n",
    "    def bits(i: int) -> tuple(\"bit {}\".format(pos) for pos in positions):\n",
    "        return ((i >> pos) & 1 for pos in positions)\n",
    "    return bits\n",
    "\n",
    "def _mod(*mods):\n",
    "    def mod(i: int) -> tuple(\"mod {}\".format(mod) for mod in mods):\n",
    "        return (i % mod for mod in mods)\n",
    "    return mod\n",
    "\n",
    "def _div(*mods):\n",
    "    def div(i: int) -> tuple(\"div {}\".format(mod) for mod in mods):\n",
    "        return (i % mod == 0 for mod in mods)\n",
    "    return div\n",
    "\n",
    "DATE_PROPS = \"tm_year\", \"tm_mon\", \"tm_mday\", \"tm_hour\", \"tm_min\", \"tm_sec\", \"tm_wday\", \"tm_yday\"\n",
    "\n",
    "@rule\n",
    "def unix2date(timestamp: int) -> DATE_PROPS:\n",
    "    t = time.gmtime(timestamp)\n",
    "    return map(lambda a: getattr(t, a), DATE_PROPS)\n",
    "\n",
    "@rule\n",
    "def unix2date_float(timestamp: float) -> DATE_PROPS:\n",
    "    return unix2date(int(timestamp))\n",
    "\n",
    "@rule\n",
    "def fracpart(x: float) -> (\"frac part\",):\n",
    "    return (x - int(x),)\n",
    "\n",
    "@rule\n",
    "def is_weekend(timestamp: int) -> (\"is weekend\",):\n",
    "    wday = time.gmtime(timestamp).tm_wday\n",
    "    wkend = wday in [5, 6]\n",
    "    return (wkend,)\n",
    "\n",
    "rule(_bits(0, 1, 2, 3, 4, 5))\n",
    "rule(_div(3, 5))\n",
    "rule(_mod(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe3308",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_bootstrap_components as dbc\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import pandas as pd\n",
    "from dash.dependencies import Input, Output, State\n",
    "from dash import dash_table\n",
    "import os\n",
    "import math\n",
    "import json\n",
    "# from data_loader import load_files, analyze_files, load_file\n",
    "# from data_profiler import DataProfiler\n",
    "# from error_detection import apply_rules, extend_dataframe_with_rules\n",
    "\n",
    "# Load the datasets\n",
    "root_directory_path = 'C:/Users/Isha/ERP/DataRepo1/'\n",
    "df_summary = analyze_files(root_directory_path)\n",
    "\n",
    "# Extract the folder name dynamically\n",
    "folder_name = os.path.basename(os.path.normpath(root_directory_path))\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP],suppress_callback_exceptions=True)\n",
    "server = app.server\n",
    "\n",
    "# Layout of the main app\n",
    "app.layout = html.Div([\n",
    "    dcc.Location(id='url', refresh=False),\n",
    "    html.Div(id='page-content')\n",
    "])\n",
    "\n",
    "# Index page layout\n",
    "# Index page layout\n",
    "index_page = html.Div([\n",
    "    dbc.Row(\n",
    "        dbc.Col(\n",
    "            html.H1(\"Data Quality Dashboard\", style={'width': '100%', 'whiteSpace': 'normal'}),\n",
    "            width={'size': 6, 'offset': 3}\n",
    "        ),\n",
    "    ),\n",
    "    dbc.Row(\n",
    "        dbc.Col(html.Hr(), width={'size': 8, 'offset': 2}),\n",
    "    ),\n",
    "    dbc.Row(\n",
    "        dbc.Col(\n",
    "            html.Div([\n",
    "                html.H4(\"Data Repo Summary\", style={'width': '100%', 'whiteSpace': 'normal'}),\n",
    "            ], style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'width': '100%'}),\n",
    "        )\n",
    "    ),\n",
    "    dbc.Row(\n",
    "        dbc.Col(\n",
    "            html.Div([\n",
    "                html.Div(f\"Name: {folder_name}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'whiteSpace': 'normal'}),\n",
    "                html.Div(f\"Size: {df_summary['size (MB)'].sum()} MB\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'whiteSpace': 'normal'}),\n",
    "                html.Div(f\"Number of Files: {len(df_summary)}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'whiteSpace': 'normal'}),\n",
    "                html.Div(f\"Path: {root_directory_path}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'whiteSpace': 'normal'}),\n",
    "                html.Div(f\"Min File Size: {df_summary['size (MB)'].min()} MB\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'whiteSpace': 'normal'}),\n",
    "                html.Div(f\"Max File Size: {df_summary['size (MB)'].max()} MB\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'whiteSpace': 'normal'})\n",
    "            ], style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'display': 'flex', 'flexDirection': 'row', 'width': '100%'}),\n",
    "        )\n",
    "    ),\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.Label('Sort by:', style={'width': '100%', 'flex': '1'}),\n",
    "            dcc.Dropdown(\n",
    "                id='sort_by',\n",
    "                options=[\n",
    "                    {'label': 'Name', 'value': 'name'},\n",
    "                    {'label': 'Size', 'value': 'size (MB)'},\n",
    "                    {'label': 'Rows', 'value': 'rows'},\n",
    "                    {'label': 'Columns', 'value': 'columns'},\n",
    "                    {'label': 'Completeness Score', 'value': 'completeness_score'},\n",
    "                    {'label': 'Outlier Percentage', 'value': 'outlier_percentage'},\n",
    "                    {'label': 'Uniqueness Score', 'value': 'uniqueness_score'},\n",
    "                    {'label': 'Data Type Distribution', 'value': 'data_type_distribution'},\n",
    "                    {'label': 'Data Quality Percent', 'value': 'data_quality_percent'},\n",
    "                    {'label': 'Ranking', 'value': 'ranking'}\n",
    "                ],\n",
    "                value='data_quality_percent',\n",
    "                style = {'width' : '100%'}\n",
    "            ),\n",
    "        ], width=4,style={'display': 'flex', 'flexDirection': 'row', 'flexWrap': 'wrap'}),\n",
    "        dbc.Col([\n",
    "            html.Label('File Type filter:', style={'width': '100%', 'whiteSpace': 'normal'}),\n",
    "            dcc.Dropdown(\n",
    "                id='file_type_filter',\n",
    "                options=[{'label': ext, 'value': ext} for ext in df_summary['extension'].unique()],\n",
    "                value=None,\n",
    "                multi=True\n",
    "            )\n",
    "        ], width=4, style={'width': '100%', 'flex': '1'})\n",
    "    ], style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px', 'display': 'flex', 'flexDirection': 'row', 'flexWrap': 'wrap'}),\n",
    "    dbc.Row(\n",
    "        dbc.Col(\n",
    "            dash_table.DataTable(\n",
    "                id='data_table',\n",
    "                columns=[\n",
    "                    {'name': 'Name', 'id': 'name'},\n",
    "                    {'name': 'Size (MB)', 'id': 'size (MB)'},\n",
    "                    {'name': 'Volume', 'id': 'volume'},\n",
    "                    {'name': 'Completeness Score', 'id': 'completeness_score'},\n",
    "                    {'name': 'Outlier Percentage', 'id': 'outlier_percentage'},\n",
    "                    {'name': 'Uniqueness Score', 'id': 'uniqueness_score'},\n",
    "                    {'name': 'Data Type Distribution', 'id': 'data_type_distribution'},\n",
    "                    {'name': 'Data Quality Score', 'id': 'data_quality_percent'},\n",
    "                    {'name': 'Ranking', 'id': 'ranking'}\n",
    "                ],\n",
    "                data=df_summary.to_dict('records'),\n",
    "                filter_action=\"native\",\n",
    "                sort_action=\"native\",\n",
    "                row_selectable=\"single\",\n",
    "                selected_rows=[],\n",
    "                page_action=\"native\",\n",
    "                page_size=10,\n",
    "                style_table={'overflowX': 'auto', 'overflowY': 'auto', 'maxHeight': '500px'},  \n",
    "                style_cell={\n",
    "                    'height': 'auto',\n",
    "                    'minWidth': '100px', 'width': 'auto', 'maxWidth': '500px',\n",
    "                    'whiteSpace': 'normal',  # Enables text wrapping\n",
    "                    'textAlign': 'left',\n",
    "                    'overflow': 'hidden',\n",
    "                    'textOverflow': 'ellipsis'\n",
    "                }\n",
    "            ), width={'size': 10, 'offset': 1}\n",
    "        ),\n",
    "    ),\n",
    "    dbc.Row([\n",
    "        dbc.Col(\n",
    "            dbc.Button(\"View Details\", id='view-details-btn', color='primary', disabled=True, href='/details'),\n",
    "            width={'size': 2, 'offset': 5}\n",
    "        ),\n",
    "        dbc.Col(\n",
    "            dbc.Button(\"Back to Index\", id='back-to-index-btn', color='secondary', href='/'),\n",
    "            style={'display': 'none'}\n",
    "        )]\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "# Detail page layout\n",
    "detail_page = dbc.Container([\n",
    "    dbc.Row([\n",
    "        dbc.Col([\n",
    "            html.Div([\n",
    "                # Title for the navigation section\n",
    "                html.H6(\"Attributes\", className=\"display-2\", style={'padding': '10px 0', 'textAlign': 'center','fontSize': '24px','fontWeight': 'bold',  }),\n",
    "                dbc.Nav(id='attribute-nav', vertical=True, pills=True, style={'overflowY':'auto'}),\n",
    "        ])], width=2, className=\"bg-light\",style={'overflowY':'auto'}),\n",
    "        dbc.Col([\n",
    "            html.Div([\n",
    "                html.Div(id='file-summary-header', style={ 'whiteSpace': 'normal'}),\n",
    "                html.Div(\n",
    "                    id='file-summary-details',\n",
    "                    style={\n",
    "                        'padding': '10px',\n",
    "                        'border': '1px solid black',\n",
    "                        'margin-bottom': '20px',\n",
    "                        'width': '100%',\n",
    "                        'whiteSpace': 'normal',\n",
    "                        'display': 'flex',\n",
    "                        'flexDirection': 'row'\n",
    "                    }\n",
    "                ),\n",
    "            ], style={'display': 'flex', 'flexDirection': 'column', 'width': '100%'}),\n",
    "            dcc.Loading(\n",
    "                html.Div(id='attribute-content', style={'width': '100%', 'whiteSpace': 'normal'})\n",
    "            )\n",
    "        ], width=10)\n",
    "    ], style={'minHeight': '100vh', 'display': 'flex', 'flexDirection': 'row'}),\n",
    "    dbc.Row([\n",
    "        dbc.Col(\n",
    "            dbc.Button(\"Back to Index\", id='back-to-index-btn', color='secondary', href='/'),\n",
    "            width={'size': 2, 'offset': 5},\n",
    "            style={'margin-top': '20px'}\n",
    "        ),\n",
    "        dbc.Col(\n",
    "            dbc.Button(\"View Details\", id='view-details-btn', color='primary', disabled=True, href='/details'),\n",
    "            style={'display':'none'}\n",
    "        )]\n",
    "    )\n",
    "], fluid=True)  # Set the container to fluid to cover full width\n",
    "\n",
    "\n",
    "\n",
    "# Update index layout based on the URL\n",
    "@app.callback(\n",
    "    Output('page-content', 'children'),\n",
    "    Input('url', 'pathname'))\n",
    "def display_page(pathname):\n",
    "    if pathname == '/details':\n",
    "        return detail_page\n",
    "    else:\n",
    "        return index_page\n",
    "\n",
    "# Define callback to update table based on filters\n",
    "@app.callback(\n",
    "    Output('data_table', 'data'),\n",
    "    Input('sort_by', 'value'),\n",
    "    Input('file_type_filter', 'value')\n",
    ")\n",
    "def update_table(sort_by, file_type_filter):\n",
    "    filtered_df = df_summary.copy()\n",
    "    if file_type_filter:\n",
    "        # Filter based on selected extensions\n",
    "        filtered_df = filtered_df[filtered_df['extension'].isin(file_type_filter)]\n",
    "\n",
    "    if sort_by:\n",
    "        filtered_df = filtered_df.sort_values(by=sort_by)\n",
    "        df_summary = filtered_df\n",
    "\n",
    "    return filtered_df.to_dict('records')\n",
    "\n",
    "# Enable/disable view details button based on row selection\n",
    "@app.callback(\n",
    "    Output('view-details-btn', 'disabled'),\n",
    "    Input('data_table', 'selected_rows')\n",
    ")\n",
    "def enable_view_details_btn(selected_rows):\n",
    "    return not bool(selected_rows)\n",
    "\n",
    "@app.callback(\n",
    "    [Output('url', 'pathname'),\n",
    "     Output('url', 'search')],\n",
    "    [Input('view-details-btn', 'n_clicks'),\n",
    "     Input('back-to-index-btn', 'n_clicks')],\n",
    "    [State('data_table', 'selected_rows')]\n",
    ")\n",
    "def handle_navigation(view_details_clicks, back_to_index_clicks, selected_rows):\n",
    "    # Handle navigation to details page\n",
    "    print(selected_rows)\n",
    "    if view_details_clicks and selected_rows:\n",
    "        selected_index = selected_rows[0]  # Get the index of the selected row\n",
    "        return '/details', f'?index={selected_index}'  # Pass the index as a query parameter\n",
    "\n",
    "    # Handle navigation back to index page\n",
    "    if back_to_index_clicks:\n",
    "        return '/', ''  # Clear query parameters\n",
    "\n",
    "    # Default return when no action is taken\n",
    "    return dash.no_update, dash.no_update\n",
    "\n",
    "# @app.callback(\n",
    "#     Output('url', 'pathname'),\n",
    "#     Output('url', 'search'),\n",
    "#     Input('view-details-btn', 'n_clicks'),\n",
    "#     State('data_table', 'selected_rows')\n",
    "# )\n",
    "# def view_details(n_clicks, selected_rows):\n",
    "#     if n_clicks and selected_rows:\n",
    "#         selected_index = selected_rows[0]  # Get the index of the selected row\n",
    "#         return '/details', f'?index={selected_index}'  # Pass the index as a query parameter\n",
    "#     return dash.no_update, dash.no_update\n",
    "\n",
    "# @app.callback(\n",
    "#     Output('url', 'pathname'),\n",
    "#     Input('back-to-index-btn', 'n_clicks')\n",
    "# )\n",
    "# def go_back(n_clicks):\n",
    "#     if n_clicks > 0:\n",
    "#         return '/'\n",
    "#     return dash.no_update\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    [Output('file-summary-header', 'children'),\n",
    "     Output('file-summary-details', 'children'),\n",
    "     Output('attribute-nav', 'children')],\n",
    "    [Input('url', 'search')]\n",
    ")\n",
    "def display_dataset_details(search):\n",
    "    # Parse the query string\n",
    "    params = urllib.parse.parse_qs(urllib.parse.urlparse(search).query)\n",
    "    index = int(params.get('index', [''])[0])\n",
    "    if index >= 0 and index < len(df_summary):\n",
    "        selected_file = df_summary.iloc[index]\n",
    "        print(selected_file)\n",
    "        df = load_single_file(os.path.join(root_directory_path, selected_file['name']))\n",
    "#         print(df)\n",
    "        \n",
    "        if not (DetectHeader(df)):\n",
    "            if df is not None:\n",
    "                df.columns = [f'Column {i+1}' for i in range(df.shape[1])]\n",
    "        \n",
    "                file_name = os.path.basename(selected_file['name'])\n",
    "                size_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n",
    "                attributes = df.columns.tolist()\n",
    "                file_format = os.path.splitext(file_name)[1].lower() \n",
    "\n",
    "                filename =  html.H6(f\"File Summary: {file_name}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "\n",
    "                nav_links = [dbc.NavLink(col, href=f\"/details?index={index}&attr={col}\", id=f\"sidebar-{col}\") for col in attributes]\n",
    "\n",
    "                summary_details = [\n",
    "                    html.Div(children = [\n",
    "                    html.Div(f\"Name: {file_name}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                    html.Div(f\"Size: {size_mb:.2f} MB\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                    html.Div(f\"File Format: {file_format}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                    html.Div(f\"Total Completeness: {df_summary.iloc[index].completeness_score:.2f}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                    html.Div(f\"Total Uniqueness: {df_summary.iloc[index].uniqueness_score:.2f}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                    ], style = {'display' : 'flex', 'flexDirection': 'row'})]\n",
    "\n",
    "                return filename, html.Div(summary_details), nav_links\n",
    "\n",
    "            return '', [], []\n",
    "\n",
    "\n",
    "# Update the detailed view based on selected attribute\n",
    "@app.callback(Output('attribute-content', 'children'),[Input('url', 'search')]\n",
    ")\n",
    "def update_attribute_content(search):\n",
    "    # Parse the query string\n",
    "    params = urllib.parse.parse_qs(urllib.parse.urlparse(search).query)\n",
    "    index = int(params.get('index', [''])[0])\n",
    "    attribute = params.get('attr', [''])[0]\n",
    "    if attribute is not None:\n",
    "        attribute = unquote(attribute)\n",
    "    print(df_summary.iloc[index])\n",
    "    if index >= 0 and index < len(df_summary):\n",
    "        selected_file = df_summary.iloc[index]\n",
    "        df = load_single_file(os.path.join(root_directory_path, selected_file['name']))\n",
    "\n",
    "        if  not (DetectHeader(df)):\n",
    "            if df is not None:\n",
    "                df.columns = [f'Column {i+1}' for i in range(df.shape[1])]\n",
    "                if attribute is None:\n",
    "                    attribute = df.columns[0]  \n",
    "\n",
    "\n",
    "                if attribute not in df.columns:\n",
    "                    return html.Div(\"Invalid attribute attribute\")\n",
    "\n",
    "                attribute_data = df[attribute]\n",
    "                outliers = pd.DataFrame()\n",
    "                profiler = DataProfiler(df, attribute)\n",
    "                x_min, x_max , y_max = 0, 0, 0\n",
    "                max_categories = 0\n",
    "            \n",
    "                print(\"column 1 is \",attribute)\n",
    "\n",
    "                if pd.api.types.is_numeric_dtype(attribute_data):\n",
    "                    # Plot numeric data\n",
    "                    line_fig = px.histogram(attribute_data, title=f'Distribution of {attribute}')\n",
    "                    graph_fig = line_fig\n",
    "        #             print(\"debug\")\n",
    "                    # Detect and display outliers\n",
    "        #             print(attribute)\n",
    "                    outliers = profiler.detect_outliers_iqr(df, attribute)\n",
    "                    # Add sliders for histogram\n",
    "                    x_min, x_max = attribute_data.min(), attribute_data.max()\n",
    "                    y_max = attribute_data.value_counts().max()\n",
    "                    # **Calculate skew and kurtosis**\n",
    "                    column_skew = skew(attribute_data.dropna())  # **Skew**\n",
    "                    column_kurtosis = kurtosis(attribute_data.dropna())  # **Kurtosis**\n",
    "\n",
    "                else:\n",
    "                    # Plot categorical data\n",
    "                    value_counts = attribute_data.value_counts()\n",
    "                    pie = px.pie(values=value_counts.values, names=value_counts.index, title=f'Distribution of {attribute}')\n",
    "                    graph_fig = pie\n",
    "        #             print(\"debug\")\n",
    "                    # Detect and display outliers\n",
    "                    outliers = profiler.detect_outliers_frequency(df, attribute)\n",
    "\n",
    "                    # Slider for top N categories with label\n",
    "                    max_categories = len(value_counts)\n",
    "\n",
    "        #         print(outliers)\n",
    "\n",
    "                sliders = html.Div([\n",
    "                    html.Div(id = \"numericSlider\",children = [ \n",
    "                        html.Label(\"X-Axis Range:\"),\n",
    "                        dcc.RangeSlider(\n",
    "                            id='x-range-slider',\n",
    "                            min=x_min,\n",
    "                            max=x_max,\n",
    "                            step=(x_max - x_min) / 100,  # Divide into 100 steps\n",
    "                            value=[x_min, x_max],\n",
    "                            marks={i: str(i) for i in np.linspace(x_min, x_max, num=5)}\n",
    "                        ),\n",
    "                        html.Label(\"Y-Axis Range:\"),\n",
    "                        dcc.RangeSlider(\n",
    "                            id='y-range-slider',\n",
    "                            min=0,\n",
    "                            max=y_max,\n",
    "                            step=y_max / 100,  # Divide into 100 steps\n",
    "                            value=[0, y_max],\n",
    "                            marks={i: str(int(i)) for i in np.linspace(0, y_max, num=5)}\n",
    "                        )\n",
    "                    ]),\n",
    "                    html.Div(id = \"categoricalSlider\",children = [\n",
    "                        html.Label(\"Top N Categories:\"),\n",
    "                        dcc.Slider(\n",
    "                            id='top-n-slider',\n",
    "                            min=1,\n",
    "                            max=max_categories,\n",
    "                            step=1,\n",
    "                            value=min(5, max_categories),  # Default to top 5 categories\n",
    "                            marks={i: str(i) for i in range(1, max_categories + 1)}\n",
    "                        )\n",
    "                    ])\n",
    "                ])\n",
    "\n",
    "                column_profile = {\n",
    "                    'Completeness': profiler.completeness_score(),\n",
    "                    'ApproxCountDistinct': profiler.uniqueness_score(),\n",
    "                    'Mean': df[attribute].mean() if pd.api.types.is_numeric_dtype(attribute_data) else 'N/A',\n",
    "                    'Minimum': df[attribute].min() if pd.api.types.is_numeric_dtype(attribute_data) else 'N/A',\n",
    "                    'Maximum': df[attribute].max() if pd.api.types.is_numeric_dtype(attribute_data) else 'N/A',\n",
    "                    'StandardDeviation': df[attribute].std() if pd.api.types.is_numeric_dtype(attribute_data) else 'N/A',\n",
    "                    'Sum': df[attribute].sum() if pd.api.types.is_numeric_dtype(attribute_data) else 'N/A',\n",
    "                    'Count': df[attribute].count(),\n",
    "                    'IsIndex': profiler.isIndex(),\n",
    "                    'MeanMedianImbalance' : profiler.mean_median_difference()}\n",
    "\n",
    "                completeness_table = dbc.Table.from_dataframe(pd.DataFrame({\n",
    "                    'Metric': ['Completeness Score', 'Uniqueness Score', 'No of Outliers'],\n",
    "                    'Value': [column_profile['Completeness'], column_profile['ApproxCountDistinct'], outliers.shape[0]]\n",
    "                }), striped=True, bordered=True, hover=True)\n",
    "\n",
    "                dataDesc = dbc.Table.from_dataframe(pd.DataFrame({\n",
    "                    \"Metric\": [\"Mean\", \"Minimum\", \"Maximum\", \"Standard Deviation\", \"Sum\", \"Count\", \"Mean/Median Imbalance\"],\n",
    "                    \"Value\": [column_profile['Mean'], column_profile['Minimum'], column_profile['Maximum'], column_profile['StandardDeviation'], column_profile['Sum'], column_profile['Count'], column_profile['MeanMedianImbalance']]\n",
    "                }), striped=True, bordered=True, hover=True)\n",
    "\n",
    "                 # Filter out the outliers for the selected attribute\n",
    "        #         outliers_for_attribute = outliers[outliers['Attribute'] == attribute]\n",
    "\n",
    "                # Prepare outliers table\n",
    "                outliers_table = dbc.Table.from_dataframe(outliers, striped=True, bordered=True, hover=True)\n",
    "\n",
    "                # Display outliers if any\n",
    "                outlier_info = html.Div()\n",
    "                if not outliers.empty:\n",
    "                    outlier_info = html.Div([\n",
    "                        html.H6(f\"Potential Erroneous values in '{attribute}' : \", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                        outliers_table\n",
    "                    ])\n",
    "\n",
    "                 # **Display skew and kurtosis if the data is numeric**\n",
    "                skew_kurtosis_info = html.Div()\n",
    "                if pd.api.types.is_numeric_dtype(attribute_data):\n",
    "                    skew_kurtosis_info = html.Div([\n",
    "                        html.H6(f\"Skew: {column_skew:.2f}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '10px'}),\n",
    "                        html.H6(f\"Kurtosis: {column_kurtosis:.2f}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'})\n",
    "                    ])\n",
    "\n",
    "                return html.Div([\n",
    "                    html.H6(f\"Details for Attribute: '{attribute}'\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                    html.H6(f\"Is '{attribute}' a Potential Index Column: {'Yes' if column_profile['IsIndex'] else 'No'}\", style={'padding': '10px', 'border': '1px solid black', 'margin-bottom': '20px'}),\n",
    "                    dbc.Row([\n",
    "                        dbc.Col(html.Div([\n",
    "                            html.H6(f\"Distribution of {attribute}\"),\n",
    "                            dcc.Graph(id='graph', figure=graph_fig,\n",
    "                            config={\n",
    "                                    'scrollZoom': True,    # Enables zooming with the scroll wheel\n",
    "                                    'displayModeBar': True,  # Displays the mode bar (tool bar)\n",
    "                                    'displaylogo': False,  # Hides the Plotly logo\n",
    "                                    'modeBarButtonsToRemove': ['select2d', 'lasso2d'],  # Customizing mode bar buttons\n",
    "                                    'staticPlot': False  # Allows full interactivity\n",
    "                                })\n",
    "                        ]), width=12)\n",
    "                    ]),\n",
    "                    sliders,\n",
    "                    dbc.Row([\n",
    "                        dbc.Col(html.Div([\n",
    "                            html.H6(\"Data Description\"),\n",
    "                            dataDesc,\n",
    "                        ]), width=6),\n",
    "                        dbc.Col([\n",
    "                            skew_kurtosis_info\n",
    "                        ], width=6)\n",
    "                    ]),\n",
    "                    html.H6(\"Completeness Table\"),\n",
    "                    completeness_table,\n",
    "                    outlier_info\n",
    "                ])\n",
    "\n",
    "    \n",
    "\n",
    "@app.callback([Output('numericSlider', 'style'),Output('categoricalSlider', 'style')],[Input('url', 'search')]\n",
    ")\n",
    "def update_sliders_visibility(search):\n",
    "#     print(\"visibility\")\n",
    "    params = urllib.parse.parse_qs(urllib.parse.urlparse(search).query)\n",
    "    index = int(params.get('index', [''])[0])\n",
    "    attribute = params.get('attr', [''])[0]\n",
    "    if attribute is not None:\n",
    "        attribute = unquote(attribute)\n",
    "\n",
    "    if index >= 0 and index < len(df_summary):\n",
    "        selected_file = df_summary.iloc[index]\n",
    "        df = load_single_file(os.path.join(root_directory_path, selected_file['name']))\n",
    "        \n",
    "        if  not (DetectHeader(df)):\n",
    "            if df is not None:\n",
    "                df.columns = [f'Column {i+1}' for i in range(df.shape[1])]\n",
    "                \n",
    "                if attribute is None : \n",
    "                    attribute = df.columns[0]\n",
    "        \n",
    "                if attribute in df.columns:\n",
    "                    attribute_data = df[attribute]\n",
    "        #             print(attribute_data)\n",
    "                    if pd.api.types.is_numeric_dtype(attribute_data):\n",
    "        #                 print(\"entered numeric in visibility \" )\n",
    "                        return [{'display': 'block'}, {'display': 'none'}]\n",
    "                    else:\n",
    "                        return [{'display': 'none'},{'display': 'block'}]\n",
    "\n",
    "    return [{'display': 'none'}, {'display': 'none'}]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "@app.callback(\n",
    "    Output('graph', 'figure'),\n",
    "    [Input('x-range-slider', 'value'),\n",
    "     Input('y-range-slider', 'value'),\n",
    "    Input('top-n-slider' , 'value')],\n",
    "    [State('url', 'search')]\n",
    ")\n",
    "def update_graph(x_range, y_range, top_n, search):\n",
    "    ctx = callback_context\n",
    "    triggered_id = ctx.triggered[0]['prop_id'].split('.')[0] if ctx.triggered else 'None'\n",
    "\n",
    "    params = urllib.parse.parse_qs(urllib.parse.urlparse(search).query)\n",
    "    index = int(params.get('index', [''])[0])\n",
    "    attribute = params.get('attr', [''])[0]\n",
    "    if attribute is not None:\n",
    "        attribute = unquote(attribute)\n",
    "\n",
    "    if index >= 0 and index < len(df_summary):\n",
    "        selected_file = df_summary.iloc[index]\n",
    "        df = load_single_file(os.path.join(root_directory_path, selected_file['name']))\n",
    "        \n",
    "        if  not (DetectHeader(df)):\n",
    "            if df is not None:\n",
    "                df.columns = [f'Column {i+1}' for i in range(df.shape[1])]\n",
    "                if attribute is None : \n",
    "                    attribute = df.columns[0]\n",
    "        \n",
    "                if attribute not in df.columns:\n",
    "                    return dash.no_update\n",
    "\n",
    "                attribute_data = df[attribute]\n",
    "\n",
    "                if pd.api.types.is_numeric_dtype(attribute_data):\n",
    "                    if 'x-range-slider' in triggered_id or 'y-range-slider' in triggered_id:\n",
    "                        filtered_data = attribute_data[(attribute_data >= x_range[0]) & (attribute_data <= x_range[1])]\n",
    "                        hist_fig = px.histogram(filtered_data, title=f'Distribution of {attribute}')\n",
    "                        hist_fig.update_layout(yaxis_range=y_range)\n",
    "                        return hist_fig\n",
    "\n",
    "                else:\n",
    "                # Handle categorical data\n",
    "                    # Limit data to top_n categories and include 'Others'\n",
    "                    value_counts = attribute_data.value_counts()\n",
    "                    top_categories = value_counts.head(top_n)\n",
    "                    other_count = value_counts.iloc[top_n:].sum()\n",
    "\n",
    "                    if other_count > 0:\n",
    "                        top_categories = top_categories.append(pd.Series([other_count], index=['Others']))\n",
    "\n",
    "        #             print(\"top categories - \", top_categories)\n",
    "\n",
    "                    pie_fig = px.pie(values=top_categories.values, names=top_categories.index, title=f'Distribution of {attribute}')\n",
    "                    return pie_fig\n",
    "\n",
    "    return dash.no_update\n",
    "\n",
    "     \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3d8769",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
